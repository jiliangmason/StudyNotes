searchkick elasticsearch要删除所有的index重来, 用curl -X DELETE 'http://localhost:9200/_all'

安装
------
下载java，各种方式(download brew apt)装完elasticsearch之后

cd elasticsearch-<version>
./bin/elasticsearch  # -d可以开守护进程

测试是否安装成功
curl 'http://localhost:9200/?pretty' #或者postman get一下

这就意味着你现在已经启动并运行一个 Elasticsearch 节点了，你可以用它做实验了。 单个 节点 可以作为一个运行中的 Elasticsearch 的实例。 而一个 集群 是一组拥有相同 cluster.name 的节点， 他们能一起工作并共享数据，还提供容错与可伸缩性。(当然，一个单独的节点也可以组成一个集群) 你可以在 elasticsearch.yml 配置文件中 修改 cluster.name ，该文件会在节点启动时加载 (译者注：这个重启服务后才会生效)。

elastic用http json api就可以交互，可以用各种语言客户端实现，或者直接curl

curl -X<VERB> '<PROTOCOL>://<HOST>:<PORT>/<PATH>?<QUERY_STRING>' -d '<BODY>'

VERB

适当的 HTTP 方法 或 谓词 : GET`、 `POST`、 `PUT`、 `HEAD 或者 `DELETE`。

PROTOCOL

http 或者 https`（如果你在 Elasticsearch 前面有一个 `https 代理）

HOST

Elasticsearch 集群中任意节点的主机名，或者用 localhost 代表本地机器上的节点。

PORT

运行 Elasticsearch HTTP 服务的端口号，默认是 9200 。

PATH

API 的终端路径（例如 _count 将返回集群中文档数量）。Path 可能包含多个组件，例如：_cluster/stats 和 _nodes/stats/jvm 。

QUERY_STRING

任意可选的查询字符串参数 (例如 ?pretty 将格式化地输出 JSON 返回值，使其更容易阅读)

BODY

一个 JSON 格式的请求体 (如果请求需要的话)

--------------------
例如想统计所有数量
curl -XGET 'http://localhost:9200/_count?pretty' -d '
{
  "query": {
          "match_all": {}
              
  }
}
'

可以给 curl -i 获取返回头信息


-------------------------
Elasticsearch存储过程叫index, 像sql 的insert, 关系型数据库通过增加一个 索引 比如一个 B树（B-tree）索引 到指定的列上，以便提升数据检索速度。Elasticsearch 和 Lucene 使用了一个叫做 倒排索引 的结构来达到相同的目的。

+ 默认的，一个文档中的每一个属性都是 被索引 的（有一个倒排索引）和可搜索的。一个没有倒排索引的属性是不能被搜索到的。

官方Example: 
对于雇员目录，我们将做如下操作：

每个雇员索引一个文档，包含该雇员的所有信息。
每个文档都将是 employee 类型 。
该类型位于 索引 megacorp 内。
该索引保存在我们的 Elasticsearch 集群中。
实践中这非常简单（尽管看起来有很多步骤），我们可以通过一条命令完成所有这些动作：

PUT /megacorp/employee/1
{
	"first_name" : "John",
	"last_name" :  "Smith",
	"age" :        25,
	"about" :      "I love to go rock climbing",
	"interests": [ "sports", "music"  ]
}

注意，路径 /megacorp/employee/1 包含了三部分的信息：

megacorp
	索引名称
employee
	类型名称
1
	特定雇员的ID

简单查询 search?q=name:Smith

GET /megacorp/employee/_search
{
  "query" : {
    "match" : {
                "last_name" : "Smith"
                        
    }
        
  }

}

现在尝试下更复杂的搜索。 同样搜索姓氏为 Smith 的雇员，但这次我们只需要年龄大于 30 的。查询需要稍作调整，使用过滤器 filter ，它支持高效地执行一个结构化查询。

GET /megacorp/employee/_search
{
	"query" : {
		"bool": {
			"must": {
				"match" : {
					"last_name" : "smith" 
				}

			},
				"filter": {
					"range" : {
						"age" : { "gt" : 30  } 
					}
				}
		}
	}
}

GET /megacorp/employee/_search
{
	"query" : {
		"match" : {
			"about" : "rock climbing"
		}
	}
}
结果
{
	...
		"hits": {
			"total":      2,
				"max_score":  0.16273327,
				"hits": [
				{
					...
						"_score":         0.16273327, 
					"_source": {
						"first_name":  "John",
						"last_name":   "Smith",
						"age":         25,
						"about":       "I love to go rock climbing",
						"interests": [ "sports", "music"  ]
					}
				},
				{
					...
						"_score":         0.016878016, 
					"_source": {
						"first_name":  "Jane",
						"last_name":   "Smith",
						"age":         32,
						"about":       "I like to collect rock albums",
						"interests": [ "music"  ]
					}
				}
			]
		}
}

Elasticsearch 默认按照相关性得分排序，即每个文档跟查询的匹配程度。第一个最高得分的结果很明显：John Smith 的 about 属性清楚地写着 “rock climbing” 。

但为什么 Jane Smith 也作为结果返回了呢？原因是她的 about 属性里提到了 “rock” 。因为只有 “rock” 而没有 “climbing” ，所以她的相关性得分低于 John 的。

这是一个很好的案例，阐明了 Elasticsearch 如何 在 全文属性上搜索并返回相关性最强的结果。Elasticsearch中的 相关性 概念非常重要，也是完全区别于传统关系型数据库的一个概念，数据库中的一条记录要么匹配要么不匹配。

-------------------------------
短语搜索

找出一个属性中的独立单词是没有问题的，但有时候想要精确匹配一系列单词或者短语 。 比如， 我们想执行这样一个查询，仅匹配同时包含 “rock” 和 “climbing” ，并且 二者以短语 “rock climbing” 的形式紧挨着的雇员记录。 为此对 match 查询稍作调整，使用一个叫做 match_phrase 的查询：

GET /megacorp/employee/_search
{
	"query" : {
		"match_phrase" : {
			"about" : "rock climbing"
		}
	}
}



-------------------------------
highlight 高亮搜索
许多应用都倾向于在每个搜索结果中 高亮 部分文本片段，以便让用户知道为何该文档符合查询条件。在 Elasticsearch 中检索出高亮片段也很容易。

再次执行前面的查询，并增加一个新的 highlight 参数：

GET /megacorp/employee/_search
{
	"query" : {
		"match_phrase" : {
			"about" : "rock climbing"
		}
	},
	"highlight": {
		"fields" : {
			"about" : {}
		}
	}
}
-------------------------------
分析
终于到了最后一个业务需求：支持管理者对雇员目录做分析。 Elasticsearch 有一个功能叫聚合（aggregations），允许我们基于数据生成一些精细的分析结果。聚合与 SQL 中的 GROUP BY 类似但更强大。

举个例子，挖掘出雇员中最受欢迎的兴趣爱好：

GET /megacorp/employee/_search
{
	"aggs": {
		"all_interests": {
  	"terms": { "field": "interests"  }
		}
	}
}
-------------------------------
数据输入输出详细

索引，存入
通过使用 index API ，文档可以被 索引 —— 存储和使文档可被搜索 。 但是首先，我们要确定文档的位置。正如我们刚刚讨论的，一个文档的 _index 、 _type 和 _id 唯一标识一个文档。 我们可以提供自定义的 _id 值，或者让 index API 自动生成。

使用自定义的 ID编辑
如果你的文档有一个自然的 标识符 （例如，一个 user_account 字段或其他标识文档的值），你应该使用如下方式的 index API 并提供你自己 _id ：

PUT /{index}/{type}/{id}
{
  "field": "value",
    ...
}
举个例子，如果我们的索引称为 website ，类型称为 blog ，并且选择 123 作为 ID ，那么索引请求应该是下面这样：

PUT /website/blog/123
{
	"title": "My first blog entry",
	"text":  "Just trying this out...",
	"date":  "2014/01/01"
}

Elasticsearch 响应体如下所示：

{
	"_index":    "website",
	"_type":     "blog",
	"_id":       "123",
	"_version":  1,
	"created":   true
}
该响应表明文档已经成功创建，该索引包括 _index 、 _type 和 _id 元数据， 以及一个新元素： _version 。

Autogenerating IDs编辑
如果你的数据没有自然的 ID， Elasticsearch 可以帮我们自动生成 ID 。 请求的结构调整为： 不再使用 PUT 谓词(“使用这个 URL 存储这个文档”)， 而是使用 POST 谓词(“存储文档在这个 URL 命名空间下”)。

现在该 URL 只需包含 _index 和 _type :

POST /website/blog/
{
  "title": "My second blog entry",
	"text":  "Still trying this out...",
	"date":  "2014/01/01"
}
拷贝为 CURL在 SENSE 中查看 
除了 _id 是 Elasticsearch 自动生成的，响应的其他部分和前面的类似：

{
   "_index":    "website",
	 "_type":     "blog",
	 "_id":       "AVFgSgVHUP18jI2wRx0w",
	 "_version":  1,
	 "created":   true
}
自动生成的 ID 是 URL-safe、 基于 Base64 编码且长度为20个字符的 GUID 字符串。 这些 GUID 字符串由可修改的 FlakeID 模式生成，这种模式允许多个节点并行生成唯一 ID ，且互相之间的冲突概率几乎为零。

-------------------------------
取回一个文档
为了从 Elasticsearch 中检索出文档 ，我们仍然使用相同的 _index , _type , 和 _id ，但是 HTTP 谓词 更改为 GET :

GET /website/blog/123?pretty
拷贝为 CURL在 SENSE 中查看 
响应体包括目前已经熟悉了的元数据元素，再加上 _source 字段，这个字段包含我们索引数据时发送给 Elasticsearch 的原始 JSON 文档：

{
	"_index" :   "website",
	"_type" :    "blog",
	"_id" :      "123",
	"_version" : 1,
	"found" :    true,
	"_source" :  {
		"title": "My first blog entry",
		"text":  "Just trying this out...",
		"date":  "2014/01/01"
	}
}
注意
在请求的查询串参数中加上 pretty 参数， 正如前面的例子中看到的，这将会调用 Elasticsearch 的 pretty-print 功能，该功能 使得 JSON 响应体更加可读。但是， _source 字段不能被格式化打印出来。相反，我们得到的 _source 字段中的 JSON 串，刚好是和我们传给它的一样。

返回文档的一部分编辑
默认情况下， GET 请求 会返回整个文档，这个文档正如存储在 _source 字段中的一样。但是也许你只对其中的 title 字段感兴趣。单个字段能用 _source 参数请求得到，多个字段也能使用逗号分隔的列表来指定。

GET /website/blog/123?_source=title,text
拷贝为 CURL在 SENSE 中查看 
该 _source 字段现在包含的只是我们请求的那些字段，并且已经将 date 字段过滤掉了。

{
	"_index" :   "website",
	"_type" :    "blog",
	"_id" :      "123",
	"_version" : 1,
	"found" :   true,
	"_source" : {
		"title": "My first blog entry" ,
		"text":  "Just trying this out..."
	}
}
或者，如果你只想得到 _source 字段，不需要任何元数据，你能使用 _source 端点：

GET /website/blog/123/_source
拷贝为 CURL在 SENSE 中查看 
那么返回的的内容如下所示：

{
	"title": "My first blog entry",
	"text":  "Just trying this out...",
	"date":  "2014/01/01"
}
-------------------------------
更新整个文档编辑
在 Elasticsearch 中文档是 不可改变 的，不能修改它们。 相反，如果想要更新现有的文档，需要 重建索引 或者进行替换， 我们可以使用相同的 index API 进行实现，在 索引文档 中已经进行了讨论。

PUT /website/blog/123
{
	"title": "My first blog entry",
	"text":  "I am starting to get the hang of this...",
	"date":  "2014/01/02"
}
在响应体中，我们能看到 Elasticsearch 已经增加了 _version 字段值：

{
	"_index" :   "website",
	"_type" :    "blog",
	"_id" :      "123",
	"_version" : 2,
	"created":   false 
}


created 标志设置成 false ，是因为相同的索引、类型和 ID 的文档已经存在。

在内部，Elasticsearch 已将旧文档标记为已删除，并增加一个全新的文档。 尽管你不能再对旧版本的文档进行访问，但它并不会立即消失。当继续索引更多的数据，Elasticsearch 会在后台清理这些已删除文档。

在本章的后面部分，我们会介绍 update API, 这个 API 可以用于 partial updates to a document 。 虽然它似乎对文档直接进行了修改，但实际上 Elasticsearch 按前述完全相同方式执行以下过程：

从旧文档构建 JSON
更改该 JSON
删除旧文档
索引一个新文档
唯一的区别在于, update API 仅仅通过一个客户端请求来实现这些步骤，而不需要单独的 get 和 index 请求。
-------------------------------
删除文档编辑
删除文档 的语法和我们所知道的规则相同，只是 使用 DELETE 方法：

DELETE /website/blog/123
拷贝为 CURL在 SENSE 中查看 
如果找到该文档，Elasticsearch 将要返回一个 200 ok 的 HTTP 响应码，和一个类似以下结构的响应体。注意，字段 _version 值已经增加:

{
	"found" :    true,
	"_index" :   "website",
	"_type" :    "blog",
	"_id" :      "123",
	"_version" : 3
}
如果文档没有 找到，我们将得到 404 Not Found 的响应码和类似这样的响应体：

{
	"found" :    false,
	"_index" :   "website",
	"_type" :    "blog",
	"_id" :      "123",
	"_version" : 4
}
即使文档不存在（ Found 是 false ）， _version 值仍然会增加。这是 Elasticsearch 内部记录本的一部分，用来确保这些改变在跨多节点时以正确的顺序执行。

注意
正如已经在更新整个文档中提到的，删除文档不会立即将文档从磁盘中删除，只是将文档标记为已删除状态。随着你不断的索引更多的数据，Elasticsearch 将会在后台清理标记为已删除的文档。
-------------------------------
乐观并发控制
Elasticsearch 是分布式的。当文档创建、更新或删除时， 新版本的文档必须复制到集群中的其他节点。Elasticsearch 也是异步和并发的，这意味着这些复制请求被并行发送，并且到达目的地时也许 顺序是乱的 。 Elasticsearch 需要一种方法确保文档的旧版本不会覆盖新的版本。

当我们之前讨论 index ， GET 和 delete 请求时，我们指出每个文档都有一个 _version （版本）号，当文档被修改时版本号递增。 Elasticsearch 使用这个 _version 号来确保变更以正确顺序得到执行。如果旧版本的文档在新版本之后到达，它可以被简单的忽略。

我们可以利用 _version 号来确保 应用中相互冲突的变更不会导致数据丢失。我们通过指定想要修改文档的 version 号来达到这个目的。 如果该版本不是当前版本号，我们的请求将会失败。

让我们创建一个新的博客文章：

PUT /website/blog/1/_create
{
	"title": "My first blog entry",
	"text":  "Just trying this out..."
}
响应体告诉我们，这个新创建的文档 _version 版本号是 1 。现在假设我们想编辑这个文档：我们加载其数据到 web 表单中， 做一些修改，然后保存新的版本。

首先我们检索文档:

GET /website/blog/1
响应体包含相同的 _version 版本号 1 ：

{
	"_index" :   "website",
	"_type" :    "blog",
	"_id" :      "1",
	"_version" : 1,
	"found" :    true,
	"_source" :  {
		"title": "My first blog entry",
		"text":  "Just trying this out..."
	}
}
现在，当我们尝试通过重建文档的索引来保存修改，我们指定 version 为我们的修改会被应用的版本：

PUT /website/blog/1?version=1 
{
	"title": "My first blog entry",
	"text":  "Starting to get the hang of this..."
}


我们想这个在我们索引中的文档只有现在的 _version 为 1 时，本次更新才能成功。

此请求成功，并且响应体告诉我们 _version 已经递增到 2 ：

{
	"_index":   "website",
	"_type":    "blog",
	"_id":      "1",
	"_version": 2
	"created":  false
}
然而，如果我们重新运行相同的索引请求，仍然指定 version=1 ， Elasticsearch 返回 409 Conflict HTTP 响应码，和一个如下所示的响应体：

{
	"error": {
	"root_cause": [
	{
		"type": "version_conflict_engine_exception",
			"reason": "[blog][1]: version conflict, current [2], provided [1]",
			"index": "website",
			"shard": "3"
	}

	],
	"type": "version_conflict_engine_exception",
	"reason": "[blog][1]: version conflict, current [2], provided [1]",
	"index": "website",
	"shard": "3"
},
	"status": 409
}
这告诉我们在 Elasticsearch 中这个文档的当前 _version 号是 2 ，但我们指定的更新版本号为 1 。

我们现在怎么做取决于我们的应用需求。我们可以告诉用户说其他人已经修改了文档，并且在再次保存之前检查这些修改内容。 或者，在之前的商品 stock_count 场景，我们可以获取到最新的文档并尝试重新应用这些修改。

所有文档的更新或删除 API，都可以接受 version 参数，这允许你在代码中使用乐观的并发控制，这是一种明智的做法。

通过外部系统使用版本控制编辑
一个常见的设置是使用其它数据库作为主要的数据存储，使用 Elasticsearch 做数据检索， 这意味着主数据库的所有更改发生时都需要被复制到 Elasticsearch ，如果多个进程负责这一数据同步，你可能遇到类似于之前描述的并发问题。

如果你的主数据库已经有了版本号 — 或一个能作为版本号的字段值比如 timestamp — 那么你就可以在 Elasticsearch 中通过增加 version_type=external 到查询字符串的方式重用这些相同的版本号， 版本号必须是大于零的整数， 且小于 9.2E+18 — 一个 Java 中 long 类型的正值。

外部版本号的处理方式和我们之前讨论的内部版本号的处理方式有些不同， Elasticsearch 不是检查当前 _version 和请求中指定的版本号是否相同， 而是检查当前 _version 是否 小于 指定的版本号。 如果请求成功，外部的版本号作为文档的新 _version 进行存储。

外部版本号不仅在索引和删除请求是可以指定，而且在 创建 新文档时也可以指定。

例如，要创建一个新的具有外部版本号 5 的博客文章，我们可以按以下方法进行：

PUT /website/blog/2?version=5&version_type=external
{
	"title": "My first external blog entry",
	"text":  "Starting to get the hang of this..."
}
在响应中，我们能看到当前的 _version 版本号是 5 ：

{
	"_index":   "website",
	"_type":    "blog",
	"_id":      "2",
	"_version": 5,
	"created":  true
}
现在我们更新这个文档，指定一个新的 version 号是 10 ：

PUT /website/blog/2?version=10&version_type=external
{
	"title": "My first external blog entry",
	"text":  "This is a piece of cake..."
}
请求成功并将当前 _version 设为 10 ：

{
	"_index":   "website",
	"_type":    "blog",
	"_id":      "2",
	"_version": 10,
	"created":  false
}
如果你要重新运行此请求时，它将会失败，并返回像我们之前看到的同样的冲突错误， 因为指定的外部版本号不大于 Elasticsearch 的当前版本号。
-------------------------------
Elasticsearch 的速度已经很快了，但甚至能更快。 将多个请求合并成一个，避免单独处理每个请求花费的网络延时和开销。 如果你需要从 Elasticsearch 检索很多文档，那么使用 multi-get 或者 mget API 来将这些检索请求放在一个请求中，将比逐个文档请求更快地检索到全部文档。

mget API 要求有一个 docs 数组作为参数，每个 元素包含需要检索文档的元数据， 包括 _index 、 _type 和 _id 。如果你想检索一个或者多个特定的字段，那么你可以通过 _source 参数来指定这些字段的名字：

GET /_mget
{
	"docs" : [
		{
			"_index" : "website",
				"_type" :  "blog",
				"_id" :    2

		},
		{
			"_index" : "website",
			"_type" :  "pageviews",
			"_id" :    1,
			"_source": "views"
		}
	]
}
该响应体也包含一个 docs 数组 ， 对于每一个在请求中指定的文档，这个数组中都包含有一个对应的响应，且顺序与请求中的顺序相同。 其中的每一个响应都和使用单个 get request 请求所得到的响应体相同：

{
	"docs" : [
		{
			"_index" :   "website",
				"_id" :      "2",
				"_type" :    "blog",
				"found" :    true,
				"_source" : {
					"text" :  "This is a piece of cake...",
					"title" : "My first external blog entry"
				},
				"_version" : 10
		},
		{
			"_index" :   "website",
			"_id" :      "1",
			"_type" :    "pageviews",
			"found" :    true,
			"_version" : 2,
			"_source" : {
				"views" : 2
			}
		}
	]
}
拷贝为 CURL在 SENSE 中查看 
如果想检索的数据都在相同的 _index 中（甚至相同的 _type 中），则可以在 URL 中指定默认的 /_index 或者默认的 /_index/_type 。

你仍然可以通过单独请求覆盖这些值：

GET /website/blog/_mget
{
	"docs" : [
		{ "_id" : 2  },
		{ "_type" : "pageviews", "_id" :   1  }
	]
}
事实上，如果所有文档的 _index 和 _type 都是相同的，你可以只传一个 ids 数组，而不是整个 docs 数组：

GET /website/blog/_mget
{
	"ids" : [ "2", "1"  ]
}
注意，我们请求的第二个文档是不存在的。我们指定类型为 blog ，但是文档 ID 1 的类型是 pageviews ，这个不存在的情况将在响应体中被报告：

{
  "docs" : [
		{
			"_index" :   "website",
			"_type" :    "blog",
			"_id" :      "2",
			"_version" : 10,
			"found" :    true,
			"_source" : {
				"title":   "My first external blog entry",
				"text":    "This is a piece of cake..."
			}
		},
		{
			"_index" :   "website",
			"_type" :    "blog",
			"_id" :      "1",
			"found" :    false  
		}
	]
}
-------------------------------
搜索详细
Elasticsearch 不只会存储（stores） 文档，为了能被搜索到也会为文档添加索引（indexes） ，这也是为什么我们使用结构化的 JSON 文档，而不是无结构的二进制数据。

文档中的每个字段都将被索引并且可以被查询 。不仅如此，在简单查询时，Elasticsearch 可以使用 所有（all） 这些索引字段，以惊人的速度返回结果。这是你永远不会考虑用传统数据库去做的一些事情。

搜索（search） 可以做到：

在类似于 gender 或者 age 这样的字段 上使用结构化查询，join_date 这样的字段上使用排序，就像SQL的结构化查询一样。
全文检索，找出所有匹配关键字的文档并按照相关性（relevance） 排序后返回结果。
以上二者兼而有之。
很多搜索都是开箱即用的，为了充分挖掘 Elasticsearch 的潜力，你需要理解以下三个概念：

映射（Mapping）
描述数据在每个字段内如何存储
分析（Analysis）
全文是如何处理使之可以被搜索的
领域特定查询语言（Query DSL）
Elasticsearch 中强大灵活的查询语言
-------------------------------
空搜索
搜索API的最基础的形式是没有指定任何查询的空搜索 ，它简单地返回集群中所有索引下的所有文档：

GET /_search
拷贝为 CURL在 SENSE 中查看 
返回的结果（为了界面简洁编辑过的）像这样：

{
	"hits" : {
	"total" :       14,
		"hits" : [
		{
			"_index":   "us",
			"_type":    "tweet",
			"_id":      "7",
			"_score":   1,
			"_source": {
				"date":    "2014-09-17",
				"name":    "John Smith",
				"tweet":   "The Query DSL is really powerful and flexible",
				"user_id": 2

			}

		},
		... 9 RESULTS REMOVED ...

		],
		"max_score" :   1

},
	"took" :           4,
	"_shards" : {
		"failed" :      0,
		"successful" :  10,
		"total" :       10

	},
	"timed_out" :      false
}
hits编辑
返回结果中最重要的部分是 hits ，它 包含 total 字段来表示匹配到的文档总数，并且一个 hits 数组包含所查询结果的前十个文档。

在 hits 数组中每个结果包含文档的 _index 、 _type 、 _id ，加上 _source 字段。这意味着我们可以直接从返回的搜索结果中使用整个文档。这不像其他的搜索引擎，仅仅返回文档的ID，需要你单独去获取文档。

每个结果还有一个 _score ，它衡量了文档与查询的匹配程度。默认情况下，首先返回最相关的文档结果，就是说，返回的文档是按照 _score 降序排列的。在这个例子中，我们没有指定任何查询，故所有的文档具有相同的相关性，因此对所有的结果而言 1 是中性的 _score 。

max_score 值是与查询所匹配文档的 _score 的最大值。

took编辑
took 值告诉我们执行整个搜索请求耗费了多少毫秒。

shards编辑
_shards 部分 告诉我们在查询中参与分片的总数，以及这些分片成功了多少个失败了多少个。正常情况下我们不希望分片失败，但是分片失败是可能发生的。如果我们遭遇到一种灾难级别的故障，在这个故障中丢失了相同分片的原始数据和副本，那么对这个分片将没有可用副本来对搜索请求作出响应。假若这样，Elasticsearch 将报告这个分片是失败的，但是会继续返回剩余分片的结果。

timeout编辑
timed_out 值告诉我们查询是否超时。默认情况下，搜索请求不会超时。 如果低响应时间比完成结果更重要，你可以指定 timeout 为 10 或者 10ms（10毫秒），或者 1s（1秒）：

GET /_search?timeout=10ms
在请求超时之前，Elasticsearch 将会返回已经成功从每个分片获取的结果。

警告
应当注意的是 timeout 不是停止执行查询，它仅仅是告知正在协调的节点返回到目前为止收集的结果并且关闭连接。在后台，其他的分片可能仍在执行查询即使是结果已经被发送了。

使用超时是因为 SLA(服务等级协议)对你是很重要的，而不是因为想去中止长时间运行的查询。

-------------------------------
多索引，多类型编辑
你有没有注意到之前的 empty search 的结果，不同类型的文档 — user 和 tweet 来自不同的索引— us 和 gb ？

如果不对某一特殊的索引或者类型做限制，就会搜索集群中的所有文档。Elasticsearch 转发搜索请求到每一个主分片或者副本分片，汇集查询出的前10个结果，并且返回给我们。

然而，经常的情况下，你 想在一个或多个特殊的索引并且在一个或者多个特殊的类型中进行搜索。我们可以通过在URL中指定特殊的索引和类型达到这种效果，如下所示：

/_search
在所有的索引中搜索所有的类型
/gb/_search
在 gb 索引中搜索所有的类型
/gb,us/_search
在 gb 和 us 索引中搜索所有的文档
/g*,u*/_search
在任何以 g 或者 u 开头的索引中搜索所有的类型
/gb/user/_search
在 gb 索引中搜索 user 类型
/gb,us/user,tweet/_search
在 gb 和 us 索引中搜索 user 和 tweet 类型
/_all/user,tweet/_search
在所有的索引中搜索 user 和 tweet 类型
当在单一的索引下进行搜索的时候，Elasticsearch 转发请求到索引的每个分片中，可以是主分片也可以是副本分片，然后从每个分片中收集结果。多索引搜索恰好也是用相同的方式工作的--只是会涉及到更多的分片。

提示
搜索一个索引有五个主分片和搜索五个索引各有一个分片准确来所说是等价的。

接下来，你将明白这种简单的方式如何灵活的根据需求的变化让扩容变得简单。

-------------------------------
分页
在之前的 空搜索 中说明了集群中有 14 个文档匹配了（empty）query 。 但是在 hits 数组中只有 10 个文档。如何才能看到其他的文档？

和 SQL 使用 LIMIT 关键字返回单个 page 结果的方法相同，Elasticsearch 接受 from 和 size 参数：

size
显示应该返回的结果数量，默认是 10
from
显示应该跳过的初始结果数量，默认是 0
如果每页展示 5 条结果，可以用下面方式请求得到 1 到 3 页的结果：

GET /_search?size=5
GET /_search?size=5&from=5
GET /_search?size=5&from=10
拷贝为 CURL在 SENSE 中查看 
考虑到分页过深以及一次请求太多结果的情况，结果集在返回之前先进行排序。 但请记住一个请求经常跨越多个分片，每个分片都产生自己的排序结果，这些结果需要进行集中排序以保证整体顺序是正确的。

在分布式系统中深度分页

理解为什么深度分页是有问题的，我们可以假设在一个有 5 个主分片的索引中搜索。 当我们请求结果的第一页（结果从 1 到 10 ），每一个分片产生前 10 的结果，并且返回给 协调节点 ，协调节点对 50 个结果排序得到全部结果的前 10 个。

现在假设我们请求第 1000 页--结果从 10001 到 10010 。所有都以相同的方式工作除了每个分片不得不产生前10010个结果以外。 然后协调节点对全部 50050 个结果排序最后丢弃掉这些结果中的 50040 个结果。

可以看到，在分布式系统中，对结果排序的成本随分页的深度成指数上升。这就是 web 搜索引擎对任何查询都不要返回超过 1000 个结果的原因。

提示
在 重新索引你的数据 中解释了如何 能够 有效获取大量的文档。
-------------------------------
轻量搜索详细
有两种形式的 搜索 API：一种是 “轻量的” 查询字符串 版本，要求在查询字符串中传递所有的 参数，另一种是更完整的 请求体 版本，要求使用 JSON 格式和更丰富的查询表达式作为搜索语言。

查询字符串搜索非常适用于通过命令行做即席查询。例如，查询在 tweet 类型中 tweet 字段包含 elasticsearch 单词的所有文档：

GET /_all/tweet/_search?q=tweet:elasticsearch
拷贝为 CURL在 SENSE 中查看 
下一个查询在 name 字段中包含 john 并且在 tweet 字段中包含 mary 的文档。实际的查询就是这样

+name:john +tweet:mary
但是查询字符串参数所需要的 百分比编码 （译者注：URL编码）实际上更加难懂：

GET /_search?q=%2Bname%3Ajohn+%2Btweet%3Amary
拷贝为 CURL在 SENSE 中查看 
+ 前缀表示必须与查询条件匹配。类似地， - 前缀表示一定不与查询条件匹配。没有 + 或者 - 的所有其他条件都是可选的——匹配的越多，文档就越相关。

_all 字段编辑
这个简单搜索返回包含 mary 的所有文档：

GET /_search?q=mary
拷贝为 CURL在 SENSE 中查看 
之前的例子中，我们在 tweet 和 name 字段中搜索内容。然而，这个查询的结果在三个地方提到了 mary ：

有一个用户叫做 Mary
6条微博发自 Mary
一条微博直接 @mary
Elasticsearch 是如何在三个不同的字段中查找到结果的呢？

当索引一个文档的时候，Elasticsearch 取出所有字段的值拼接成一个大的字符串，作为 _all 字段进行索引。例如，当索引这个文档时：

{
	"tweet":    "However did I manage before Elasticsearch?",
	"date":     "2014-09-14",
	"name":     "Mary Jones",
	"user_id":  1
}
这就好似增加了一个名叫 _all 的额外字段：

"However did I manage before Elasticsearch? 2014-09-14 Mary Jones 1"
除非设置特定字段，否则查询字符串就使用 _all 字段进行搜索。

提示
在刚开始开发一个应用时，_all 字段是一个很实用的特性。之后，你会发现如果搜索时用指定字段来代替 _all 字段，将会更好控制搜索结果。当 _all 字段不再有用的时候，可以将它置为失效，正如在 元数据: _all 字段 中所解释的。

更复杂的查询编辑
下面的查询针对tweents类型，并使用以下的条件：

name 字段中包含 mary 或者 john
date 值大于 2014-09-10
_all_ 字段包含 aggregations 或者 geo
+name:(mary john) +date:>2014-09-10 +(aggregations geo)
拷贝为 CURL在 SENSE 中查看 
查询字符串在做了适当的编码后，可读性很差：

?q=%2Bname%3A(mary+john)+%2Bdate%3A%3E2014-09-10+%2B(aggregations+geo)
从之前的例子中可以看出，这种 轻量 的查询字符串搜索效果还是挺让人惊喜的。 它的查询语法在相关参考文档中有详细解释，以便简洁的表达很复杂的查询。对于通过命令做一次性查询，或者是在开发阶段，都非常方便。

但同时也可以看到，这种精简让调试更加晦涩和困难。而且很脆弱，一些查询字符串中很小的语法错误，像 - ， : ， / 或者 " 不匹配等，将会返回错误而不是搜索结果。

最后，查询字符串搜索允许任何用户在索引的任意字段上执行可能较慢且重量级的查询，这可能会暴露隐私信息，甚至将集群拖垮。

提示
因为这些原因，不推荐直接向用户暴露查询字符串搜索功能，除非对于集群和数据来说非常信任他们。

相反，我们经常在生产环境中更多地使用功能全面的 request body 查询API，除了能完成以上所有功能，还有一些附加功能。但在到达那个阶段之前，首先需要了解数据在 Elasticsearch 中是如何被索引的。"
-------------------------------
映射和分析 mapping
当摆弄索引里面的数据时，我们发现一些奇怪的事情。一些事情看起来被打乱了：在我们的索引中有12条推文，其中只有一条包含日期 2014-09-15 ，但是看一看下面查询命中的 总数 （total）：

GET /_search?q=2014              # 12 results
GET /_search?q=2014-09-15        # 12 results !
GET /_search?q=date:2014-09-15   # 1  result
GET /_search?q=date:2014         # 0  results !
拷贝为 CURL在 SENSE 中查看 
为什么在 _all 字段查询日期返回所有推文，而在 date 字段只查询年份却没有返回结果？为什么我们在 _all 字段和 date 字段的查询结果有差别？

推测起来，这是因为数据在 _all 字段与 date 字段的索引方式不同。所以，通过请求 gb 索引中 tweet 类型的_映射_（或模式定义），让我们看一看 Elasticsearch 是如何解释我们文档结构的：

GET /gb/_mapping/tweet
拷贝为 CURL在 SENSE 中查看 
这将得到如下结果：

{
	"gb": {
	"mappings": {
		"tweet": {
			"properties": {
				"date": {
					"type": "date",
						"format": "strict_date_optional_time||epoch_millis"

				},
				"name": {
					"type": "string"

				},
				"tweet": {
					"type": "string"

				},
				"user_id": {
					"type": "long"

				}

			}

		}

	}

}

}
基于对字段类型的猜测， Elasticsearch 动态为我们产生了一个映射。这个响应告诉我们 date 字段被认为是 date 类型的。由于 _all 是默认字段，所以没有提及它。但是我们知道 _all 字段是 string 类型的。

所以 date 字段和 string 字段 索引方式不同，因此搜索结果也不一样。这完全不令人吃惊。你可能会认为 核心数据类型 strings、numbers、Booleans 和 dates 的索引方式有稍许不同。没错，他们确实稍有不同。

但是，到目前为止，最大的差异在于 代表 精确值 （它包括 string 字段）的字段和代表 全文 的字段。这个区别非常重要——它将搜索引擎和所有其他数据库区别开来。
-------------------------------
当摆弄索引里面的数据时，我们发现一些奇怪的事情。一些事情看起来被打乱了：在我们的索引中有12条推文，其中只有一条包含日期 2014-09-15 ，但是看一看下面查询命中的 总数 （total）：

GET /_search?q=2014              # 12 results
GET /_search?q=2014-09-15        # 12 results !
GET /_search?q=date:2014-09-15   # 1  result
GET /_search?q=date:2014         # 0  results !
拷贝为 CURL在 SENSE 中查看 
为什么在 _all 字段查询日期返回所有推文，而在 date 字段只查询年份却没有返回结果？为什么我们在 _all 字段和 date 字段的查询结果有差别？

推测起来，这是因为数据在 _all 字段与 date 字段的索引方式不同。所以，通过请求 gb 索引中 tweet 类型的_映射_（或模式定义），让我们看一看 Elasticsearch 是如何解释我们文档结构的：

GET /gb/_mapping/tweet
拷贝为 CURL在 SENSE 中查看 
这将得到如下结果：

{
	"gb": {
	"mappings": {
		"tweet": {
			"properties": {
				"date": {
					"type": "date",
						"format": "strict_date_optional_time||epoch_millis"

				},
					"name": {
						"type": "string"

					},
					"tweet": {
						"type": "string"

					},
					"user_id": {
						"type": "long"

					}

			}

		}

	}

}

}
基于对字段类型的猜测， Elasticsearch 动态为我们产生了一个映射。这个响应告诉我们 date 字段被认为是 date 类型的。由于 _all 是默认字段，所以没有提及它。但是我们知道 _all 字段是 string 类型的。

所以 date 字段和 string 字段 索引方式不同，因此搜索结果也不一样。这完全不令人吃惊。你可能会认为 核心数据类型 strings、numbers、Booleans 和 dates 的索引方式有稍许不同。没错，他们确实稍有不同。

但是，到目前为止，最大的差异在于 代表 精确值 （它包括 string 字段）的字段和代表 全文 的字段。这个区别非常重要——它将搜索引擎和所有其他数据库区别开来。
------------------------
Elasticsearch 中的数据可以概括的分为两类：精确值和全文。

精确值 如它们听起来那样精确。例如日期或者用户 ID，但字符串也可以表示精确值，例如用户名或邮箱地址。对于精确值来讲，Foo 和 foo 是不同的，2014 和 2014-09-15 也是不同的。

另一方面，全文 是指文本数据（通常以人类容易识别的语言书写），例如一个推文的内容或一封邮件的内容。

注意
全文通常是指非结构化的数据，但这里有一个误解：自然语言是高度结构化的。问题在于自然语言的规则是复杂的，导致计算机难以正确解析。例如，考虑这条语句：

May is fun but June bores me.
它指的是月份还是人？

精确值很容易查询。结果是二进制的：要么匹配查询，要么不匹配。这种查询很容易用 SQL 表示：

WHERE name    = "John Smith"
  AND user_id = 2
	AND date    > "2014-09-15"

查询全文数据要微妙的多。我们问的不只是“这个文档匹配查询吗”，而是“该文档匹配查询的程度有多大？”换句话说，该文档与给定查询的相关性如何？

我们很少对全文类型的域做精确匹配。相反，我们希望在文本类型的域中搜索。不仅如此，我们还希望搜索能够理解我们的 意图 ：

搜索 UK ，会返回包含 United Kindom 的文档。
搜索 jump ，会匹配 jumped ， jumps ， jumping ，甚至是 leap 。
搜索 johnny walker 会匹配 Johnnie Walker ， johnnie depp 应该匹配 Johnny Depp 。
fox news hunting 应该返回福克斯新闻（ Foxs News ）中关于狩猎的故事，同时， fox hunting news 应该返回关于猎狐的故事。
为了促进这类在全文域中的查询，Elasticsearch 首先 分析 文档，之后根据结果创建 倒排索引 。在接下来的两节，我们会讨论倒排索引和分析过程。


------------------------
倒排索引
Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。

例如，假设我们有两个文档，每个文档的 content 域包含如下内容：

The quick brown fox jumped over the lazy dog
Quick brown foxes leap over lazy dogs in summer
为了创建倒排索引，我们首先将每个文档的 content 域拆分成单独的 词（我们称它为 词条 或 tokens ），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。结果如下所示：

Term      Doc_1  Doc_2
-------------------------
Quick   |       |  X
The     |   X   |
brown   |   X   |  X
dog     |   X   |
dogs    |       |  X
fox     |   X   |
foxes   |       |  X
in      |       |  X
jumped  |   X   |
lazy    |   X   |  X
leap    |       |  X
over    |   X   |  X
quick   |   X   |
summer  |       |  X
the     |   X   |
------------------------
现在，如果我们想搜索 quick brown ，我们只需要查找包含每个词条的文档：

Term      Doc_1  Doc_2
-------------------------
brown   |   X   |  X
quick   |   X   |
------------------------
Total   |   2   |  1
两个文档都匹配，但是第一个文档比第二个匹配度更高。如果我们使用仅计算匹配词条数量的简单 相似性算法 ，那么，我们可以说，对于我们查询的相关性来讲，第一个文档比第二个文档更佳。

但是，我们目前的倒排索引有一些问题：

Quick 和 quick 以独立的词条出现，然而用户可能认为它们是相同的词。
fox 和 foxes 非常相似, 就像 dog 和 dogs ；他们有相同的词根。
jumped 和 leap, 尽管没有相同的词根，但他们的意思很相近。他们是同义词。
使用前面的索引搜索 +Quick +fox 不会得到任何匹配文档。（记住，+ 前缀表明这个词必须存在。）只有同时出现 Quick 和 fox 的文档才满足这个查询条件，但是第一个文档包含 quick fox ，第二个文档包含 Quick foxes 。

我们的用户可以合理的期望两个文档与查询匹配。我们可以做的更好。

如果我们将词条规范为标准模式，那么我们可以找到与用户搜索的词条不完全一致，但具有足够相关性的文档。例如：

Quick 可以小写化为 quick 。
foxes 可以 词干提取 --变为词根的格式-- 为 fox 。类似的， dogs 可以为提取为 dog 。
jumped 和 leap 是同义词，可以索引为相同的单词 jump 。
现在索引看上去像这样：

Term      Doc_1  Doc_2
-------------------------
brown   |   X   |  X
dog     |   X   |  X
fox     |   X   |  X
in      |       |  X
jump    |   X   |  X
lazy    |   X   |  X
over    |   X   |  X
quick   |   X   |  X
summer  |       |  X
the     |   X   |  X
------------------------
这还远远不够。我们搜索 +Quick +fox 仍然 会失败，因为在我们的索引中，已经没有 Quick 了。但是，如果我们对搜索的字符串使用与 content 域相同的标准化规则，会变成查询 +quick +fox ，这样两个文档都会匹配！

注意
这非常重要。你只能搜索在索引中出现的词条，所以索引文本和查询字符串必须标准化为相同的格式。

分词和标准化的过程称为 分析 ， 我们会在下个章节讨论。
------------------------
分析器!!!!!!
分析 包含下面的过程：

首先，将一块文本分成适合于倒排索引的独立的 词条 ，
之后，将这些词条统一化为标准格式以提高它们的“可搜索性”，或者 recall
分析器执行上面的工作。 分析器 实际上是将三个功能封装到了一个包里：

字符过滤器
首先，字符串按顺序通过每个 字符过滤器 。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将 & 转化成 `and`。
分词器
其次，字符串被 分词器 分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。
Token 过滤器
最后，词条按顺序通过每个 token 过滤器 。这个过程可能会改变词条（例如，小写化 Quick ），删除词条（例如， 像 a`， `and`， `the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词）。
Elasticsearch提供了开箱即用的字符过滤器、分词器和token 过滤器。 这些可以组合起来形成自定义的分析器以用于不同的目的。我们会在 自定义分析器 章节详细讨论。

内置分析器编辑
但是， Elasticsearch还附带了可以直接使用的预包装的分析器。 接下来我们会列出最重要的分析器。为了证明它们的差异，我们看看每个分析器会从下面的字符串得到哪些词条：

"Set the shape to semi-transparent by calling set_trans(5)"
标准分析器
标准分析器是Elasticsearch默认使用的分析器。它是分析各种语言文本最常用的选择。它根据 Unicode 联盟 定义的 单词边界 划分文本。删除绝大部分标点。最后，将词条小写。它会产生

set, the, shape, to, semi, transparent, by, calling, set_trans, 5
简单分析器
简单分析器在任何不是字母的地方分隔文本，将词条小写。它会产生

set, the, shape, to, semi, transparent, by, calling, set, trans
空格分析器
空格分析器在空格的地方划分文本。它会产生

Set, the, shape, to, semi-transparent, by, calling, set_trans(5)
语言分析器
特定语言分析器可用于 很多语言。它们可以考虑指定语言的特点。例如， 英语 分析器附带了一组英语无用词（常用单词，例如 and 或者 the ，它们对相关性没有多少影响），它们会被删除。 由于理解英语语法的规则，这个分词器可以提取英语单词的 词干 。

英语 分词器会产生下面的词条：

set, shape, semi, transpar, call, set_tran, 5
注意看 transparent`、 `calling 和 set_trans 已经变为词根格式。

什么时候使用分析器编辑
当我们 索引 一个文档，它的全文域被分析成词条以用来创建倒排索引。 但是，当我们在全文域 搜索 的时候，我们需要将查询字符串通过 相同的分析过程 ，以保证我们搜索的词条格式与索引中的词条格式一致。

全文查询，理解每个域是如何定义的，因此它们可以做 正确的事：

当你查询一个 全文 域时， 会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。
当你查询一个 精确值 域时，不会分析查询字符串， 而是搜索你指定的精确值。
现在你可以理解在 开始章节 的查询为什么返回那样的结果：

date 域包含一个精确值：单独的词条 `2014-09-15`。
_all 域是一个全文域，所以分词进程将日期转化为三个词条： `2014`， `09`， 和 `15`。
当我们在 _all 域查询 2014`，它匹配所有的12条推文，因为它们都含有 `2014 ：

GET /_search?q=2014              # 12 results
拷贝为 CURL在 SENSE 中查看 
当我们在 _all 域查询 2014-09-15`，它首先分析查询字符串，产生匹配 `2014`， `09`， 或 `15 中 任意 词条的查询。这也会匹配所有12条推文，因为它们都含有 2014 ：

GET /_search?q=2014-09-15        # 12 results !
拷贝为 CURL在 SENSE 中查看 
当我们在 date 域查询 `2014-09-15`，它寻找 精确 日期，只找到一个推文：

GET /_search?q=date:2014-09-15   # 1  result
拷贝为 CURL在 SENSE 中查看 
当我们在 date 域查询 `2014`，它找不到任何文档，因为没有文档含有这个精确日志：

GET /_search?q=date:2014         # 0  results !
拷贝为 CURL在 SENSE 中查看 
测试分析器编辑
有些时候很难理解分词的过程和实际被存储到索引中的词条，特别是你刚接触 Elasticsearch。为了理解发生了什么，你可以使用 analyze API 来看文本是如何被分析的。在消息体里，指定分析器和要分析的文本：

GET /_analyze
{
	"analyzer": "standard",
	"text": "Text to analyze"
}
拷贝为 CURL在 SENSE 中查看 
结果中每个元素代表一个单独的词条：

{
	"tokens": [
		{
			"token":        "text",
				"start_offset": 0,
				"end_offset":   4,
				"type":         "<ALPHANUM>",
				"position":     1

		},
		{
			"token":        "to",
			"start_offset": 5,
			"end_offset":   7,
			"type":         "<ALPHANUM>",
			"position":     2

		},
		{
			"token":        "analyze",
			"start_offset": 8,
			"end_offset":   15,
			"type":         "<ALPHANUM>",
			"position":     3
		}
	]
}
token 是实际存储到索引中的词条。 position 指明词条在原始文本中出现的位置。 start_offset 和 end_offset 指明字符在原始字符串中的位置。

提示
每个分析器的 type 值都不一样，可以忽略它们。它们在Elasticsearch中的唯一作用在于keep_types token 过滤器。

analyze API 是一个有用的工具，它有助于我们理解Elasticsearch索引内部发生了什么，随着深入，我们会进一步讨论它。

指定分析器编辑
当Elasticsearch在你的文档中检测到一个新的字符串域 ，它会自动设置其为一个全文 字符串 域，使用 标准 分析器对它进行分析。

你不希望总是这样。可能你想使用一个不同的分析器，适用于你的数据使用的语言。有时候你想要一个字符串域就是一个字符串域--不使用分析，直接索引你传入的精确值，例如用户ID或者一个内部的状态域或标签。

要做到这一点，我们必须手动指定这些域的映射。
------------------------
映射编辑 !!自定义mapping
为了能够将时间域视为时间，数字域视为数字，字符串域视为全文或精确值字符串， Elasticsearch 需要知道每个域中数据的类型。这个信息包含在映射中。

如 数据输入和输出 中解释的， 索引中每个文档都有 类型 。每种类型都有它自己的 映射 ，或者 模式定义 。映射定义了类型中的域，每个域的数据类型，以及Elasticsearch如何处理这些域。映射也用于配置与类型有关的元数据。

我们会在 类型和映射 详细讨论映射。本节，我们只讨论足够让你入门的内容。

核心简单域类型编辑
Elasticsearch 支持 如下简单域类型：

字符串: string
整数 : byte, short, integer, long
浮点数: float, double
布尔型: boolean
日期: date
当你索引一个包含新域的文档--之前未曾出现-- Elasticsearch 会使用 动态映射 ，通过JSON中基本数据类型，尝试猜测域类型，使用如下规则：

JSON type | 域 type

布尔型: true 或者 false | boolean

整数: 123  | long

浮点数: 123.45 | double

字符串，有效日期: 2014-09-15 | date

字符串: foo bar | string

注意
这意味着如果你通过引号( "123"  )索引一个数字，它会被映射为 string 类型，而不是 long 。但是，如果这个域已经映射为 long ，那么 Elasticsearch 会尝试将这个字符串转化为 long ，如果无法转化，则抛出一个异常。

查看映射编辑
通过 /_mapping ，我们可以查看 Elasticsearch 在一个或多个索引中的一个或多个类型的映射 。在 开始章节 ，我们已经取得索引 gb 中类型 tweet 的映射：

GET /gb/_mapping/tweet
Elasticsearch 根据我们索引的文档，为域(称为 属性 )动态生成的映射。

{
	"gb": {
		"mappings": {
			"tweet": {
				"properties": {
					"date": {
						"type": "date",
						"format": "strict_date_optional_time||epoch_millis"
					},
						"name": {
							"type": "string"
						},
						"tweet": {
							"type": "string"
						},
						"user_id": {
							"type": "long"
						}
				}
			}
		}
	}
}
提示
错误的映射，例如 将 age 域映射为 string 类型，而不是 integer ，会导致查询出现令人困惑的结果。

检查一下！而不是假设你的映射是正确的。

自定义域映射编辑
尽管在很多情况下基本域数据类型 已经够用，但你经常需要为单独域自定义映射 ，特别是字符串域。自定义映射允许你执行下面的操作：

全文字符串域和精确值字符串域的区别
使用特定语言分析器
优化域以适应部分匹配
指定自定义数据格式
还有更多
域最重要的属性是 type 。对于不是 string 的域，你一般只需要设置 type ：

{
	"number_of_clicks": {
		"type": "integer"
	}

}
默认， string 类型域会被认为包含全文。就是说，它们的值在索引前，会通过 一个分析器，针对于这个域的查询在搜索前也会经过一个分析器。

string 域映射的两个最重要 属性是 index 和 analyzer 。

index编辑
index 属性控制怎样索引字符串。它可以是下面三个值：

analyzed
首先分析字符串，然后索引它。换句话说，以全文索引这个域。
not_analyzed
  索引这个域，所以它能够被搜索，但索引的是精确值。不会对它进行分析。
  no
  不索引这个域。这个域不会被搜索到。
  string 域 index 属性默认是 analyzed 。如果我们想映射这个字段为一个精确值，我们需要设置它为 not_analyzed ：

{
	"tag": {
		"type":     "string",
		"index":    "not_analyzed"
	}
}
注意
其他简单类型（例如 long ， double ， date 等）也接受 index 参数，但有意义的值只有 no 和 not_analyzed ， 因为它们永远不会被分析。

analyzer编辑
对于 analyzed 字符串域，用 analyzer 属性指定在搜索和索引时使用的分析器。默认， Elasticsearch 使用 standard 分析器， 但你可以指定一个内置的分析器替代它，例如 whitespace 、 simple 和 `english`：

{
	"tweet": {
		"type":     "string",
		"analyzer": "english"
	}
}
在 自定义分析器 ，我们会展示怎样定义和使用自定义分析器。

更新映射编辑
当你首次 创建一个索引的时候，可以指定类型的映射。你也可以使用 /_mapping 为新类型（或者为存在的类型更新映射）增加映射。

注意
尽管你可以 增加_ 一个存在的映射，你不能 _修改 存在的域映射。如果一个域的映射已经存在，那么该域的数据可能已经被索引。如果你意图修改这个域的映射，索引的数据可能会出错，不能被正常的搜索。

我们可以更新一个映射来添加一个新域，但不能将一个存在的域从 analyzed 改为 not_analyzed 。

为了描述指定映射的两种方式，我们先删除 gd 索引：

DELETE /gb
拷贝为 CURL在 SENSE 中查看 
然后创建一个新索引，指定 tweet 域使用 english 分析器：

PUT /gb 
{
	"mappings": {
		"tweet" : {
			"properties" : {
				"tweet" : {
					"type" :    "string",
						"analyzer": "english"
				},
				"date" : {
					"type" :   "date"
				},
				"name" : {
					"type" :   "string"
				},
				"user_id" : {
					"type" :   "long"
				}
			}
		}
	}
}
拷贝为 CURL在 SENSE 中查看 


通过消息体中指定的 mappings 创建了索引。

稍后，我们决定在 tweet 映射增加一个新的名为 tag 的 not_analyzed 的文本域，使用 _mapping ：

PUT /gb/_mapping/tweet
{
	"properties" : {
		"tag" : {
		"type" :    "string",
		"index":    "not_analyzed"
		}
	}
}
拷贝为 CURL在 SENSE 中查看 
注意，我们不需要再次列出所有已存在的域，因为无论如何我们都无法改变它们。新域已经被合并到存在的映射中。

测试映射编辑
你可以使用 analyze API 测试字符串域的映射。比较下面两个请求的输出：

GET /gb/_analyze
{
	"field": "tweet",
	"text": "Black-cats" 
}

GET /gb/_analyze
{
	"field": "tag",
	"text": "Black-cats" 
}
拷贝为 CURL在 SENSE 中查看 
 

 消息体里面传输我们想要分析的文本。

 tweet 域产生两个词条 black 和 cat ， tag 域产生单独的词条 Black-cats 。换句话说，我们的映射正常工作。
------------------------
复杂核心型搜索
除了我们提到的简单标量数据类型， JSON 还有 null 值，数组，和对象，这些 Elasticsearch 都是支持的。

多值域编辑
很有可能，我们希望 tag 域 包含多个标签。我们可以以数组的形式索引标签：

{ "tag": [ "search", "nosql"  ] }
对于数组，没有特殊的映射需求。任何域都可以包含0、1或者多个值，就像全文域分析得到多个词条。

这暗示 数组中所有的值必须是相同数据类型的 。你不能将日期和字符串混在一起。如果你通过索引数组来创建新的域，Elasticsearch 会用数组中第一个值的数据类型作为这个域的 类型 。

注意
当你从 Elasticsearch 得到一个文档，每个数组的顺序和你当初索引文档时一样。你得到的 _source 域，包含与你索引的一模一样的 JSON 文档。

但是，数组是以多值域 索引的—可以搜索，但是无序的。 在搜索的时候，你不能指定 “第一个” 或者 “最后一个”。 更确切的说，把数组想象成 装在袋子里的值 。

空域编辑
当然，数组可以为空。 这相当于存在零值。 事实上，在 Lucene 中是不能存储 null 值的，所以我们认为存在 null 值的域为空域。

下面三种域被认为是空的，它们将不会被索引：

"null_value":               null,
"empty_array":              [],
"array_with_null_value":    [ null  ]
多层级对象编辑
我们讨论的最后一个 JSON 原生数据类是 对象 -- 在其他语言中称为哈希，哈希 map，字典或者关联数组。

内部对象 经常用于 嵌入一个实体或对象到其它对象中。例如，与其在 tweet 文档中包含 user_name 和 user_id 域，我们也可以这样写：

{
	"tweet":            "Elasticsearch is very flexible",
		"user": {
		"id":           "@johnsmith",
		"gender":       "male",
		"age":          26,
		"name": {
			"full":     "John Smith",
			"first":    "John",
			"last":     "Smith"
		}
	}
}
内部对象的映射编辑
Elasticsearch 会动态 监测新的对象域并映射它们为 对象 ，在 properties 属性下列出内部域：

{
	"gb": {
		"tweet": { 
			"properties": {
				"tweet":            { "type": "string"  },
				"user": { 
					"type":             "object",
					"properties": {
						"id":           { "type": "string"  },
						"gender":       { "type": "string"  },
						"age":          { "type": "long"    },
						"name":   { 
							"type":         "object",
							"properties": {
								"full":     { "type": "string"  },
								"first":    { "type": "string"  },
								"last":     { "type": "string"  }
							}
						}
					}
				}
			}
		}
	}
}


根对象

 

 内部对象

 user 和 name 域的映射结构与 tweet 类型的相同。事实上， type 映射只是一种特殊的 对象 映射，我们称之为 根对象 。除了它有一些文档元数据的特殊顶级域，例如 _source 和 _all 域，它和其他对象一样。

 内部对象是如何索引的编辑
 Lucene 不理解内部对象。 Lucene 文档是由一组键值对列表组成的。为了能让 Elasticsearch 有效地索引内部类，它把我们的文档转化成这样：

{
 "tweet":            [elasticsearch, flexible, very],
 "user.id":          [@johnsmith],
 "user.gender":      [male],
 "user.age":         [26],
 "user.name.full":   [john, smith],
 "user.name.first":  [john],
 "user.name.last":   [smith]
}
内部域 可以通过名称引用（例如， first ）。为了区分同名的两个域，我们可以使用全 路径 （例如， user.name.first ） 或 type 名加路径（ tweet.user.name.first ）。

注意
在前面简单扁平的文档中，没有 user 和 user.name 域。Lucene 索引只有标量和简单值，没有复杂数据结构。

内部对象数组编辑
最后，考虑包含 内部对象的数组是如何被索引的。 假设我们有个 followers 数组：

{
	"followers": [
		{ "age": 35, "name": "Mary White" },
		{ "age": 26, "name": "Alex Jones" },
		{ "age": 19, "name": "Lisa Smith" }
	]
}
这个文档会像我们之前描述的那样被扁平化处理，结果如下所示：

{
	"followers.age":    [19, 26, 35],
	"followers.name":   [alex, jones, lisa, smith, mary, white]
}
{age: 35} 和 {name: Mary White} 之间的相关性已经丢失了，因为每个多值域只是一包无序的值，而不是有序数组。这足以让我们问，“有一个26岁的追随者？”

但是我们不能得到一个准确的答案：“是否有一个26岁 名字叫 Alex Jones 的追随者？”

相关内部对象被称为 nested 对象，可以回答上面的查询，我们稍后会在嵌套对象中介绍它。
------------------------
使用FullBody Search
查询表达式(Query DSL)是一种非常灵活又富有表现力的 查询语言。 Elasticsearch 使用它可以以简单的 JSON 接口来展现 Lucene 功能的绝大部分。在你的应用中，你应该用它来编写你的查询语句。它可以使你的查询语句更灵活、更精确、易读和易调试。

要使用这种查询表达式，只需将查询语句传递给 query 参数：

GET /_search
{
	"query": YOUR_QUERY_HERE
}
空查询（empty search） —{}— 在功能上等价于使用 match_all 查询， 正如其名字一样，匹配所有文档：

GET /_search
{
	"query": {
		"match_all": {}
	}
}
拷贝为 CURL在 SENSE 中查看 
查询语句的结构编辑
一个查询语句 的典型结构：

{
	QUERY_NAME: {
		ARGUMENT: VALUE,
		ARGUMENT: VALUE,...
	}
}
如果是针对某个字段，那么它的结构如下：

{
	QUERY_NAME: {
		FIELD_NAME: {
			ARGUMENT: VALUE,
			ARGUMENT: VALUE,...
		}
	}
}
举个例子，你可以使用 match 查询语句 来查询 tweet 字段中包含 elasticsearch 的 tweet：

{
	"match": {
		"tweet": "elasticsearch"
	}
}
完整的查询请求如下：

GET /_search
{
	"query": {
		"match": {
			"tweet": "elasticsearch"
		}
	}
}
拷贝为 CURL在 SENSE 中查看 
合并查询语句编辑
查询语句(Query clauses) 就像一些简单的组合块 ，这些组合块可以彼此之间合并组成更复杂的查询。这些语句可以是如下形式：

叶子语句（Leaf clauses） (就像 match 语句) 被用于将查询字符串和一个字段（或者多个字段）对比。
复合(Compound) 语句 主要用于 合并其它查询语句。 比如，一个 bool 语句 允许在你需要的时候组合其它语句，无论是 must 匹配、 must_not 匹配还是 should 匹配，同时它可以包含不评分的过滤器（filters）：
{
	"bool": {
		"must":     { "match": { "tweet": "elasticsearch"  } },
		"must_not": { "match": { "name":  "mary"  } },
		"should":   { "match": { "tweet": "full text"  } },
		"filter":   { "range": { "age" : { "gt" : 30  } }  }
	}
}
拷贝为 CURL在 SENSE 中查看 
一条复合语句可以合并 任何 其它查询语句，包括复合语句，了解这一点是很重要的。这就意味着，复合语句之间可以互相嵌套，可以表达非常复杂的逻辑。

例如，以下查询是为了找出信件正文包含 business opportunity 的星标邮件，或者在收件箱正文包含 business opportunity 的非垃圾邮件：

{
	"bool": {
		"must": { "match":   { "email": "business opportunity"  } },
		"should": [
			{ "match":       { "starred": true  } },
			{ "bool": {
				"must":      { "match": { "folder": "inbox"  } },
				"must_not":  { "match": { "spam": true  } }
			} }
		],
		"minimum_should_match": 1
	}
}
到目前为止，你不必太在意这个例子的细节，我们会在后面详细解释。最重要的是你要理解到，一条复合语句可以将多条语句 — 叶子语句和其它复合语句 — 合并成一个单一的查询语句

------------------------
比较重要的几个查询
虽然 Elasticsearch 自带了很多的查询，但经常用到的也就那么几个。我们将在 深入搜索 章节详细讨论那些查询的细节，接下来我们对最重要的几个查询进行简单介绍。

match_all 查询编辑
match_all 查询简单的 匹配所有文档。在没有指定查询方式时，它是默认的查询：

{ "match_all": {} }
拷贝为 CURL在 SENSE 中查看 
它经常与 filter 结合使用--例如，检索收件箱里的所有邮件。所有邮件被认为具有相同的相关性，所以都将获得分值为 1 的中性 `_score`。

match 查询编辑
无论你在任何字段上进行的是全文搜索还是精确查询，match 查询是你可用的标准查询。

如果你在一个全文字段上使用 match 查询，在执行查询前，它将用正确的分析器去分析查询字符串：

{ "match": { "tweet": "About Search"  } }
拷贝为 CURL在 SENSE 中查看 
如果在一个精确值的字段上使用它， 例如数字、日期、布尔或者一个 not_analyzed 字符串字段，那么它将会精确匹配给定的值：

{ "match": { "age":    26            } }
{ "match": { "date":   "2014-09-01"  } }
{ "match": { "public": true          } }
{ "match": { "tag":    "full_text"   } }
拷贝为 CURL在 SENSE 中查看 
提示
对于精确值的查询，你可能需要使用 filter 语句来取代 query，因为 filter 将会被缓存。接下来，我们将看到一些关于 filter 的例子。

不像我们在 轻量 搜索 章节介绍的字符串查询（query-string search）， match 查询不使用类似 +user_id:2 +tweet:search 的查询语法。它只是去查找给定的单词。这就意味着将查询字段暴露给你的用户是安全的；你需要控制那些允许被查询字段，不易于抛出语法异常。

multi_match 查询编辑
multi_match 查询可以在多个字段上执行相同的 match 查询：

{
	"multi_match": {
		"query":    "full text search",
		"fields":   [ "title", "body"  ]
	}
}
拷贝为 CURL在 SENSE 中查看 
range 查询编辑
range 查询找出那些落在指定区间内的数字或者时间：

{
	"range": {
		"age": {
			"gte":  20,
			"lt":   30
		}
	}
}
拷贝为 CURL在 SENSE 中查看 
被允许的操作符如下：

gt
大于
gte
大于等于
lt
小于
lte
小于等于
term 查询编辑
term 查询被用于精确值 匹配，这些精确值可能是数字、时间、布尔或者那些 not_analyzed 的字符串：

{ "term": { "age":    26            } }
{ "term": { "date":   "2014-09-01"  } }
{ "term": { "public": true          } }
{ "term": { "tag":    "full_text"   } }
拷贝为 CURL在 SENSE 中查看 
term 查询对于输入的文本不 分析 ，所以它将给定的值进行精确查询。

terms 查询编辑
terms 查询和 term 查询一样，但它允许你指定多值进行匹配。如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件：

{ "terms": { "tag": [ "search", "full_text", "nosql"  ]  } }
拷贝为 CURL在 SENSE 中查看 
和 term 查询一样，terms 查询对于输入的文本不分析。它查询那些精确匹配的值（包括在大小写、重音、空格等方面的差异）。

exists 查询和 missing 查询编辑
exists 查询和 missing 查询被用于查找那些指定字段中有值 (exists) 或无值 (missing) 的文档。这与SQL中的 IS_NULL (missing) 和 NOT IS_NULL (exists) 在本质上具有共性：

{
	"exists":   {
		"field":    "title"
	}
}
拷贝为 CURL在 SENSE 中查看 
这些查询经常用于某个字段有值的情况和某个字段缺值的情况。
------------------------
现实的查询需求从来都没有那么简单；它们需要在多个字段上查询多种多样的文本，并且根据一系列的标准来过滤。为了构建类似的高级查询，你需要一种能够将多查询组合成单一查询的查询方法。

你可以用 bool 查询来实现你的需求。这种查询将多查询组合在一起，成为用户自己想要的布尔查询。它接收以下参数：

must
文档 必须 匹配这些条件才能被包含进来。
must_not
文档 必须不 匹配这些条件才能被包含进来。
should
如果满足这些语句中的任意语句，将增加 _score ，否则，无任何影响。它们主要用于修正每个文档的相关性得分。
filter
必须 匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档。
由于这是我们看到的第一个包含多个查询的查询，所以有必要讨论一下相关性得分是如何组合的。每一个子查询都独自地计算文档的相关性得分。一旦他们的得分被计算出来， bool 查询就将这些得分进行合并并且返回一个代表整个布尔操作的得分。

下面的查询用于查找 title 字段匹配 how to make millions 并且不被标识为 spam 的文档。那些被标识为 starred 或在2014之后的文档，将比另外那些文档拥有更高的排名。如果 _两者_ 都满足，那么它排名将更高：

{
	"bool": {
		"must":     { "match": { "title": "how to make millions"  } },
		"must_not": { "match": { "tag":   "spam"  } },
		"should": [
			{ "match": { "tag": "starred"  } },
			{ "range": { "date": { "gte": "2014-01-01"  } } }
		]
	}
}
拷贝为 CURL在 SENSE 中查看 
提示
如果没有 must 语句，那么至少需要能够匹配其中的一条 should 语句。但，如果存在至少一条 must 语句，则对 should 语句的匹配没有要求。

增加带过滤器（filtering）的查询编辑
如果我们不想因为文档的时间而影响得分，可以用 filter 语句来重写前面的例子：

{
	"bool": {
		"must":     { "match": { "title": "how to make millions"  } },
		"must_not": { "match": { "tag":   "spam"  } },
		"should": [
			{ "match": { "tag": "starred"  } }
		],
		"filter": {
			"range": { "date": { "gte": "2014-01-01"  } } 
		}
	}
}
拷贝为 CURL在 SENSE 中查看 


range 查询已经从 should 语句中移到 filter 语句

通过将 range 查询移到 filter 语句中，我们将它转成不评分的查询，将不再影响文档的相关性排名。由于它现在是一个不评分的查询，可以使用各种对 filter 查询有效的优化手段来提升性能。

所有查询都可以借鉴这种方式。将查询移到 bool 查询的 filter 语句中，这样它就自动的转成一个不评分的 filter 了。

如果你需要通过多个不同的标准来过滤你的文档，bool 查询本身也可以被用做不评分的查询。简单地将它放置到 filter 语句中并在内部构建布尔逻辑：

{
	"bool": {
		"must":     { "match": { "title": "how to make millions"  } },
		"must_not": { "match": { "tag":   "spam"  } },
		"should": [
			{ "match": { "tag": "starred"  } }
		],
		"filter": {
			"bool": { 
				"must": [
					{ "range": { "date": { "gte": "2014-01-01"  } } },
					{ "range": { "price": { "lte": 29.99  } } }
				],
				"must_not": [
					{ "term": { "category": "ebooks"  } }
				]
			}
		}
	}
}
拷贝为 CURL在 SENSE 中查看 


将 bool 查询包裹在 filter 语句中，我们可以在过滤标准中增加布尔逻辑

通过混合布尔查询，我们可以在我们的查询请求中灵活地编写 scoring 和 filtering 查询逻辑。

constant_score 查询编辑
尽管没有 bool 查询使用这么频繁，constant_score 查询也是你工具箱里有用的查询工具。它将一个不变的常量评分应用于所有匹配的文档。它被经常用于你只需要执行一个 filter 而没有其它查询（例如，评分查询）的情况下。

可以使用它来取代只有 filter 语句的 bool 查询。在性能上是完全相同的，但对于提高查询简洁性和清晰度有很大帮助。

{
	"constant_score":   {
		"filter": {
			"term": { "category": "ebooks"  } 
		}
	}
}
拷贝为 CURL在 SENSE 中查看 


term 查询被放置在 constant_score 中，转成不评分的 filter。这种方式可以用来取代只有 filter 语句的 bool 查询。
------------------------
验证查询
查询可以变得非常的复杂，尤其 和不同的分析器与不同的字段映射结合时，理解起来就有点困难了。不过 validate-query API 可以用来验证查询是否合法。

GET /gb/tweet/_validate/query
{
	"query": {
		"tweet" : {
			"match" : "really powerful"
		}
	}
}
拷贝为 CURL在 SENSE 中查看 
以上 validate 请求的应答告诉我们这个查询是不合法的：

{
	"valid" :         false,
	"_shards" : {
		"total" :       1,
		"successful" :  1,
		"failed" :      0
	}
}
理解错误信息编辑
为了找出 查询不合法的原因，可以将 explain 参数 加到查询字符串中：

GET /gb/tweet/_validate/query?explain 
{
	"query": {
		"tweet" : {
			"match" : "really powerful"
		}
	}
}
拷贝为 CURL在 SENSE 中查看 


explain 参数可以提供更多关于查询不合法的信息。

很明显，我们将查询类型(match)与字段名称 (tweet)搞混了：

{
	"valid" :     false,
	"_shards" :   { ...  },
	"explanations" : [ {
		"index" :   "gb",
		"valid" :   false,
		"error" :   "org.elasticsearch.index.query.QueryParsingException:
		[gb] No query registered for [tweet]"
	}  ]
}
理解查询语句编辑
对于合法查询，使用 explain 参数将返回可读的描述，这对准确理解 Elasticsearch 是如何解析你的 query 是非常有用的：

GET /_validate/query?explain
{
	"query": {
		"match" : {
			"tweet" : "really powerful"
		}
	}
}
拷贝为 CURL在 SENSE 中查看 
我们查询的每一个 index 都会返回对应的 explanation ，因为每一个 index 都有自己的映射和分析器：

{
	"valid" :         true,
	"_shards" :       { ...  },
	"explanations" : [ {
		"index" :       "us",
		"valid" :       true,
		"explanation" : "tweet:really tweet:powerful"
	}, {
	"index" :       "gb",
	"valid" :       true,
	"explanation" : "tweet:realli tweet:power"
	}  ]

}
从 explanation 中可以看出，匹配 really powerful 的 match 查询被重写为两个针对 tweet 字段的 single-term 查询，一个single-term查询对应查询字符串分出来的一个term。

当然，对于索引 us ，这两个 term 分别是 really 和 powerful ，而对于索引 gb ，term 则分别是 realli 和 power 。之所以出现这个情况，是由于我们将索引 gb 中 tweet 字段的分析器修改为 english 分析器。
------------------------
prefix 前缀查询编辑
为了找到所有以 W1 开始的邮编，可以使用简单的 prefix 查询：

GET /my_index/address/_search
{
	"query": {
		"prefix": {
			"postcode": "W1"
		}
	}
}
拷贝为 CURL在 SENSE 中查看 
prefix 查询是一个词级别的底层的查询，它不会在搜索之前分析查询字符串，它假定传入前缀就正是要查找的前缀。

提示
默认状态下， prefix 查询不做相关度评分计算，它只是将所有匹配的文档返回，并为每条结果赋予评分值 1 。它的行为更像是过滤器而不是查询。 prefix 查询和 prefix 过滤器这两者实际的区别就是过滤器是可以被缓存的，而查询不行。

之前已经提过：“只能在倒排索引中找到存在的词”，但我们并没有对这些邮编的索引进行特殊处理，每个邮编还是以它们精确值的方式存在于每个文档的索引中，那么 prefix 查询是如何工作的呢？

回想倒排索引包含了一个有序的唯一词列表（本例是邮编）。 对于每个词，倒排索引都会将包含词的文档 ID 列入 倒排表（postings list） 。与示例对应的倒排索引是：

Term:          Doc IDs:
-------------------------
"SW5 0BE"    |  5
"W1F 7HW"    |  3
"W1V 3DG"    |  1
"W2F 8HW"    |  2
"WC1N 1LZ"   |  4
-------------------------
为了支持前缀匹配，查询会做以下事情：

扫描词列表并查找到第一个以 W1 开始的词。
搜集关联的文档 ID 。
移动到下一个词。
如果这个词也是以 W1 开头，查询跳回到第二步再重复执行，直到下一个词不以 W1 为止。
这对于小的例子当然可以正常工作，但是如果倒排索引中有数以百万的邮编都是以 W1 开头时，前缀查询则需要访问每个词然后计算结果！

前缀越短所需访问的词越多。如果我们要以 W 作为前缀而不是 W1 ，那么就可能需要做千万次的匹配。

小心
prefix 查询或过滤对于一些特定的匹配是有效的，但使用方式还是应当注意。 当字段中词的集合很小时，可以放心使用，但是它的伸缩性并不好，会对我们的集群带来很多压力。可以使用较长的前缀来限制这种影响，减少需要访问的量。

本章后面会介绍另一个索引时的解决方案，这个方案能使前缀匹配更高效，不过在此之前，需要先看看两个相关的查询： wildcard 和 regexp （模糊和正则）。
------------------------
wildcard!!!
与 prefix 前缀查询的特性类似， wildcard 通配符查询也是一种底层基于词的查询， 与前缀查询不同的是它允许指定匹配的正则式。它使用标准的 shell 通配符查询： ? 匹配任意字符， * 匹配 0 或多个字符。

这个查询会匹配包含 W1F 7HW 和 W2F 8HW 的文档：

GET /my_index/address/_search
{
  "query": {
    "wildcard": {
      "postcode": "W?F*HW" 
    }
  }
}
拷贝为 CURL在 SENSE 中查看 


? 匹配 1 和 2 ， * 与空格及 7 和 8 匹配。

设想如果现在只想匹配 W 区域的所有邮编，前缀匹配也会包括以 WC 开头的所有邮编，与通配符匹配碰到的问题类似，如果想匹配只以 W 开始并跟随一个数字的所有邮编， regexp 正则式查询允许写出这样更复杂的模式：

GET /my_index/address/_search
{
  "query": {
    "regexp": {
      "postcode": "W[0-9].+" 
    }
  }
}
拷贝为 CURL在 SENSE 中查看 


这个正则表达式要求词必须以 W 开头，紧跟 0 至 9 之间的任何一个数字，然后接一或多个其他字符。

wildcard 和 regexp 查询的工作方式与 prefix 查询完全一样
------------------------
把邮编的事情先放一边，让我们先看看前缀查询是如何在全文查询中起作用的。 用户已经渐渐习惯在输完查询内容之前，就能为他们展现搜索结果，这就是所谓的 即时搜索（instant search） 或 输入即搜索（search-as-you-type） 。不仅用户能在更短的时间内得到搜索结果，我们也能引导用户搜索索引中真实存在的结果。

例如，如果用户输入 johnnie walker bl ，我们希望在它们完成输入搜索条件前就能得到：Johnnie Walker Black Label 和 Johnnie Walker Blue Label 。

生活总是这样，就像猫的花色远不只一种！我们希望能找到一种最简单的实现方式。并不需要对数据做任何准备，在查询时就能对任意的全文字段实现 输入即搜索（search-as-you-type） 的查询。

在 短语匹配 中，我们引入了 match_phrase 短语匹配查询，它匹配相对顺序一致的所有指定词语，对于查询时的输入即搜索，可以使用 match_phrase 的一种特殊形式， match_phrase_prefix 查询：

{
  "match_phrase_prefix" : {
    "brand" : "johnnie walker bl"
  }
}

这种查询的行为与 match_phrase 查询一致，不同的是它将查询字符串的最后一个词作为前缀使用，换句话说，可以将之前的例子看成如下这样：
johnnie
跟着 walker
跟着以 bl 开始的词
如果通过 validate-query API 运行这个查询查询，explanation 的解释结果为：
"johnnie walker bl*"
与 match_phrase 一样，它也可以接受 slop 参数（参照 slop ）让相对词序位置不那么严格：

{
  "match_phrase_prefix" : {
    "brand" : {
      "query": "walker johnnie bl", 
      "slop":  10
    }
  }
}

尽管词语的顺序不正确，查询仍然能匹配，因为我们为它设置了足够高的 slop 值使匹配时的词序有更大的灵活性。
但是只有查询字符串的最后一个词才能当作前缀使用。

在之前的 前缀查询 中，我们警告过使用前缀的风险，即 prefix 查询存在严重的资源消耗问题，短语查询的这种方式也同样如此。 前缀 a 可能会匹配成千上万的词，这不仅会消耗很多系统资源，而且结果的用处也不大。

可以通过设置 max_expansions 参数来限制前缀扩展的影响， 一个合理的值是可能是 50 ：

{
  "match_phrase_prefix" : {
    "brand" : {
      "query":          "johnnie walker bl",
      "max_expansions": 50
    }
  }
}
参数 max_expansions 控制着可以与前缀匹配的词的数量，它会先查找第一个与前缀 bl 匹配的词，然后依次查找搜集与之匹配的词（按字母顺序），直到没有更多可匹配的词或当数量超过 max_expansions 时结束。
不要忘记，当用户每多输入一个字符时，这个查询又会执行一遍，所以查询需要快，如果第一个结果集不是用户想要的，他们会继续输入直到能搜出满意的结果为止。
------------------------
用Ngram处理索引时就搜索
设置索引时输入即搜索的第一步是需要定义好分析链， 我们已在 配置分析器 中讨论过，这里会对这些步骤再次说明。

准备索引编辑
第一步需要配置一个自定义的 edge_ngram token 过滤器，称为 autocomplete_filter ：

{
  "filter": {
    "autocomplete_filter": {
      "type":     "edge_ngram",
      "min_gram": 1,
      "max_gram": 20
    }
  }
}
这个配置的意思是：对于这个 token 过滤器接收的任意词项，过滤器会为之生成一个最小固定值为 1 ，最大为 20 的 n-gram 。
然后会在一个自定义分析器 autocomplete 中使用上面这个 token 过滤器：
{
  "analyzer": {
    "autocomplete": {
      "type":      "custom",
      "tokenizer": "standard",
      "filter": [
        "lowercase",
        "autocomplete_filter" 
      ]
    }
  }
}

自定义的 edge-ngram token 过滤器。
这个分析器使用 standard 分词器将字符串拆分为独立的词，并且将它们都变成小写形式，然后为每个词生成一个边界 n-gram，这要感谢 autocomplete_filter 起的作用。

创建索引、实例化 token 过滤器和分析器的完整示例如下：

PUT /my_index
{
  "settings": {
    "number_of_shards": 1, 
    "analysis": {
      "filter": {
        "autocomplete_filter": { 
          "type":     "edge_ngram",
          "min_gram": 1,
          "max_gram": 20
        }
      },
      "analyzer": {
        "autocomplete": {
          "type":      "custom",
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "autocomplete_filter" 
          ]
        }
      }
    }
  }
}
拷贝为 CURL在 SENSE 中查看 

参考 被破坏的相关度 。


首先自定义 token 过滤器。



然后在分析器中使用它。

可以拿 analyze API 测试这个新的分析器确保它行为正确：

GET /my_index/_analyze?analyzer=autocomplete
quick brown
拷贝为 CURL在 SENSE 中查看 
结果表明分析器能正确工作，并返回以下词：

q
qu
qui
quic
quick
b
br
bro
brow
brown
可以用 update-mapping API 将这个分析器应用到具体字段：

PUT /my_index/_mapping/my_type
{
  "my_type": {
    "properties": {
      "name": {
        "type":     "string",
        "analyzer": "autocomplete"
      }
    }
  }
}
现在创建一些测试文档：
POST /my_index/my_type/_bulk
{ "index": { "_id": 1            }}
{ "name": "Brown foxes"    }
{ "index": { "_id": 2            }}
{ "name": "Yellow furballs" }
拷贝为 CURL在 SENSE 中查看 
查询字段编辑
如果使用简单 match 查询测试查询 “brown fo” ：
GET /my_index/my_type/_search
{
  "query": {
    "match": {
      "name": "brown fo"
    }
  }
}
拷贝为 CURL在 SENSE 中查看 
可以看到两个文档同时 都能 匹配，尽管 Yellow furballs 这个文档并不包含 brown 和 fo ：
{
  "hits": [
    {
      "_id": "1",
      "_score": 1.5753809,
      "_source": {
        "name": "Brown foxes"
      }
    },
    {
      "_id": "2",
      "_score": 0.012520773,
      "_source": {
        "name": "Yellow furballs"
      }
    }
  ]
}
如往常一样， validate-query API 总能提供一些线索：
GET /my_index/my_type/_validate/query?explain
{
  "query": {
    "match": {
      "name": "brown fo"
    }
  }
}
explanation 表明查询会查找边界 n-grams 里的每个词：
name:b name:br name:bro name:brow name:brown name:f name:fo
name:f 条件可以满足第二个文档，因为 furballs 是以 f 、 fu 、 fur 形式索引的。回过头看这并不令人惊讶，相同的 autocomplete 分析器同时被应用于索引时和搜索时，这在大多数情况下是正确的，只有在少数场景下才需要改变这种行为。
我们需要保证倒排索引表中包含边界 n-grams 的每个词，但是我们只想匹配用户输入的完整词组（ brown 和 fo ）， 可以通过在索引时使用 autocomplete 分析器，并在搜索时使用 standard 标准分析器来实现这种想法，只要改变查询使用的搜索分析器 analyzer 参数即可：

GET /my_index/my_type/_search
{
  "query": {
    "match": {
      "name": {
        "query":    "brown fo",
        "analyzer": "standard" 
      }
    }
  }
}

覆盖了 name 字段 analyzer 的设置。
换种方式，我们可以在映射中，为 name 字段分别指定 index_analyzer 和 search_analyzer 。因为我们只想改变 search_analyzer ，这里只要更新现有的映射而不用对数据重新创建索引：

PUT /my_index/my_type/_mapping
{
  "my_type": {
    "properties": {
      "name": {
        "type":            "string",
        "index_analyzer":  "autocomplete", 
        "search_analyzer": "standard" 
      }
    }
  }
}

在索引时，使用 autocomplete 分析器生成边界 n-grams 的每个词。


在搜索时，使用 standard 分析器只搜索用户输入的词。

如果再次请求 validate-query API ，当前的解释为：

name:brown name:fo
再次执行查询就能正确返回 Brown foxes 这个文档。

因为大多数工作是在索引时完成的，所有的查询只要查找 brown 和 fo 这两个词，这比使用 match_phrase_prefix 查找所有以 fo 开始的词的方式要高效许多。

补全提示（Completion Suggester）

使用边界 n-grams 进行输入即搜索（search-as-you-type）的查询设置简单、灵活且快速，但有时候它并不够快，特别是当试图立刻获得反馈时，延迟的问题就会凸显，很多时候不搜索才是最快的搜索方式。

Elasticsearch 里的 completion suggester 采用与上面完全不同的方式，需要为搜索条件生成一个所有可能完成的词列表，然后将它们置入一个 有限状态机（finite state transducer） 内，这是个经优化的图结构。为了搜索建议提示，Elasticsearch 从图的开始处顺着匹配路径一个字符一个字符地进行匹配，一旦它处于用户输入的末尾，Elasticsearch 就会查找所有可能结束的当前路径，然后生成一个建议列表。

本数据结构存于内存中，能使前缀查找非常快，比任何一种基于词的查询都要快很多，这对名字或品牌的自动补全非常适用，因为这些词通常是以普通顺序组织的：用 “Johnny Rotten” 而不是 “Rotten Johnny” 。

当词序不是那么容易被预见时，边界 n-grams 比完成建议者（Completion Suggester）更合适。即使说不是所有猫都是一个花色，那这只猫的花色也是相当特殊的。

边界 n-grams 与邮编编辑
边界 n-gram 的方式可以用来查询结构化的数据， 比如 本章之前示例 中的邮编（postcode）。当然 postcode 字段需要 analyzed 而不是 not_analyzed ，不过可以用 keyword 分词器来处理它，就好像他们是 not_analyzed 的一样。

提示
keyword 分词器是一个非操作型分词器，这个分词器不做任何事情，它接收的任何字符串都会被原样发出，因此它可以用来处理 not_analyzed 的字段值，但这也需要其他的一些分析转换，如将字母转换成小写。

下面示例使用 keyword 分词器将邮编转换成 token 流，这样就能使用边界 n-gram token 过滤器：

{
  "analysis": {
    "filter": {
      "postcode_filter": {
        "type":     "edge_ngram",
        "min_gram": 1,
        "max_gram": 8
      }
    },
    "analyzer": {
      "postcode_index": { 
        "tokenizer": "keyword",
        "filter":    [ "postcode_filter" ]
      },
      "postcode_search": { 
        "tokenizer": "keyword"
      }
    }
  }
}

postcode_index 分析器使用 postcode_filter 将邮编转换成边界 n-gram 形式。


postcode_search 分析器可以将搜索词看成 not_analyzed 未分析的。
------------------------

kibana 6.2开始用 ./bin/kibana-plugin来安装插件
$ bin/kibana-plugin install <package name or URL>
$ bin/kibana-plugin install x-pack
$ bin/kibana-plugin install https://artifacts.elastic.co/downloads/packs/x-pack/x-pack-6.2.3.zip

---------------------
mapping里， 后面elastic已经把类型string drop掉，现在都叫text, index类型以后都是true false

---------------------
Searchkick里用高级搜索去使用wildcard
 Topic.search(body: {query: {wildcard: {name: "*a*"}}}).to_a

--------------------
Rails里searchkick为字段添加ngram搜索，首先可以看看 curl es_url/_settings, 当前支持哪些analyzer, ngram的搜索类似like wildcard, 然后model这样设置在reindex一下，就能实现即搜即得

class Topic < ApplicationRecord
  searchkick merge_mappings: true, mappings: {
    topic: {
      properties: {
        name: {
          type: "text",
          fields: {
            analyzed: {
              type: "text",
              analyzer: "searchkick_word_middle_index"
            }
          }
        }
      }
    }
  }
  #....
end
